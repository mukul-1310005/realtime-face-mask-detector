{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "oriental-worthy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "coral-density",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Flatten, AveragePooling2D, Dense, Input, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import imutils\n",
    "from imutils import paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-spectacular",
   "metadata": {},
   "source": [
    "**Importing our Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "instructional-provider",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = r\"dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-bargain",
   "metadata": {},
   "source": [
    "*setting image paths to variables with all images at once*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "color-expression",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagePaths = list(paths.list_images(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-reach",
   "metadata": {},
   "source": [
    "**Extracting Data and labels from dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "distributed-colonial",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "data = []\n",
    "\n",
    "for i in imagePaths:\n",
    "    label = i.split(os.path.sep)[-2]\n",
    "    labels.append(label)\n",
    "    image = load_img(i, target_size = (224,224))\n",
    "    image = img_to_array(image)\n",
    "    image = preprocess_input(image)\n",
    "    data.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "surprised-waterproof",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(labels)\n",
    "data = np.array(data, dtype = \"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "prime-punishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)\n",
    "labels = to_categorical(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-brunei",
   "metadata": {},
   "source": [
    "**Splitting Our Data into train Data and split Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "female-symbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size = 0.2, random_state = 10, stratify = labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marked-terminology",
   "metadata": {},
   "source": [
    "Here we will use model called MobileNet, therefore we will download it below. This model is very light weight and provide similar efficiency as VGG. We will store it in variable called baseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "pleased-bicycle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "baseModel = MobileNetV2(weights = 'imagenet', include_top = False, input_tensor = Input(shape = (224,224,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worldwide-dancing",
   "metadata": {},
   "source": [
    "**Lets make layers of our baseModel as Non Trainable because, We need its pretrained value for detection, we will ony train headModel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "protective-suspension",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in baseModel.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-departure",
   "metadata": {},
   "source": [
    "**Let us make our head of baseModel that will be trainable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "marked-packaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "headModel = baseModel.output\n",
    "headModel = AveragePooling2D(pool_size = (7,7))(headModel)\n",
    "headModel = Flatten(name = \"Flatten\")(headModel)\n",
    "headModel = Dense(128, activation = 'relu')(headModel)\n",
    "headModel = Dropout(0.4)(headModel)\n",
    "headModel = Dense(2, activation = 'softmax')(headModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "vertical-charge",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs = baseModel.input, outputs = headModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "composed-reaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image Data Generation\n",
    "aug = ImageDataGenerator(rotation_range = 50, zoom_range = 0.2, width_shift_range = 0.1, height_shift_range = 0.1, horizontal_flip = True,vertical_flip = True, fill_mode = \"nearest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-touch",
   "metadata": {},
   "source": [
    "**Training Our Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "painted-average",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "Epochs = 36\n",
    "BS = 12 #batch size\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(lr = learning_rate, decay = learning_rate/Epochs)\n",
    "model.compile(loss = \"binary_crossentropy\", optimizer = opt, metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "supposed-graph",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/36\n",
      "91/91 [==============================] - 47s 520ms/step - loss: 0.2231 - accuracy: 0.9136 - val_loss: 0.0217 - val_accuracy: 0.9928\n",
      "Epoch 2/36\n",
      "91/91 [==============================] - 46s 502ms/step - loss: 0.0479 - accuracy: 0.9825 - val_loss: 0.0142 - val_accuracy: 0.9928\n",
      "Epoch 3/36\n",
      "91/91 [==============================] - 46s 511ms/step - loss: 0.0435 - accuracy: 0.9825 - val_loss: 0.0107 - val_accuracy: 0.9964\n",
      "Epoch 4/36\n",
      "91/91 [==============================] - 46s 506ms/step - loss: 0.0631 - accuracy: 0.9825 - val_loss: 0.0159 - val_accuracy: 0.9928\n",
      "Epoch 5/36\n",
      "91/91 [==============================] - 47s 512ms/step - loss: 0.0358 - accuracy: 0.9853 - val_loss: 0.0132 - val_accuracy: 0.9964\n",
      "Epoch 6/36\n",
      "91/91 [==============================] - 46s 508ms/step - loss: 0.0350 - accuracy: 0.9890 - val_loss: 0.0165 - val_accuracy: 0.9891\n",
      "Epoch 7/36\n",
      "91/91 [==============================] - 46s 506ms/step - loss: 0.0254 - accuracy: 0.9908 - val_loss: 0.0237 - val_accuracy: 0.9891\n",
      "Epoch 8/36\n",
      "91/91 [==============================] - 46s 508ms/step - loss: 0.0184 - accuracy: 0.9954 - val_loss: 0.0276 - val_accuracy: 0.9891\n",
      "Epoch 9/36\n",
      "91/91 [==============================] - 46s 507ms/step - loss: 0.0516 - accuracy: 0.9825 - val_loss: 0.0316 - val_accuracy: 0.9891\n",
      "Epoch 10/36\n",
      "91/91 [==============================] - 46s 506ms/step - loss: 0.0292 - accuracy: 0.9871 - val_loss: 0.0154 - val_accuracy: 0.9928\n",
      "Epoch 11/36\n",
      "91/91 [==============================] - 46s 508ms/step - loss: 0.0372 - accuracy: 0.9862 - val_loss: 0.0102 - val_accuracy: 0.9928\n",
      "Epoch 12/36\n",
      "91/91 [==============================] - 46s 507ms/step - loss: 0.0445 - accuracy: 0.9853 - val_loss: 0.0192 - val_accuracy: 0.9928\n",
      "Epoch 13/36\n",
      "91/91 [==============================] - 47s 513ms/step - loss: 0.0351 - accuracy: 0.9853 - val_loss: 0.0048 - val_accuracy: 1.0000\n",
      "Epoch 14/36\n",
      "91/91 [==============================] - 46s 506ms/step - loss: 0.0215 - accuracy: 0.9926 - val_loss: 0.0079 - val_accuracy: 0.9964\n",
      "Epoch 15/36\n",
      "91/91 [==============================] - 46s 507ms/step - loss: 0.0178 - accuracy: 0.9945 - val_loss: 0.0080 - val_accuracy: 0.9964\n",
      "Epoch 16/36\n",
      "91/91 [==============================] - 47s 517ms/step - loss: 0.0236 - accuracy: 0.9908 - val_loss: 0.0083 - val_accuracy: 0.9928\n",
      "Epoch 17/36\n",
      "91/91 [==============================] - 46s 505ms/step - loss: 0.0256 - accuracy: 0.9926 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
      "Epoch 18/36\n",
      "91/91 [==============================] - 46s 506ms/step - loss: 0.0152 - accuracy: 0.9945 - val_loss: 0.0134 - val_accuracy: 0.9928\n",
      "Epoch 19/36\n",
      "91/91 [==============================] - 46s 506ms/step - loss: 0.0437 - accuracy: 0.9844 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
      "Epoch 20/36\n",
      "91/91 [==============================] - 46s 506ms/step - loss: 0.0251 - accuracy: 0.9908 - val_loss: 0.0175 - val_accuracy: 0.9891\n",
      "Epoch 21/36\n",
      "91/91 [==============================] - 46s 506ms/step - loss: 0.0477 - accuracy: 0.9862 - val_loss: 0.0101 - val_accuracy: 0.9928\n",
      "Epoch 22/36\n",
      "91/91 [==============================] - 46s 505ms/step - loss: 0.0322 - accuracy: 0.9881 - val_loss: 0.0041 - val_accuracy: 1.0000\n",
      "Epoch 23/36\n",
      "91/91 [==============================] - 47s 514ms/step - loss: 0.0099 - accuracy: 0.9982 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 24/36\n",
      "91/91 [==============================] - 46s 507ms/step - loss: 0.0212 - accuracy: 0.9917 - val_loss: 0.0048 - val_accuracy: 1.0000\n",
      "Epoch 25/36\n",
      "91/91 [==============================] - 46s 508ms/step - loss: 0.0214 - accuracy: 0.9926 - val_loss: 0.0039 - val_accuracy: 1.0000\n",
      "Epoch 26/36\n",
      "91/91 [==============================] - 47s 515ms/step - loss: 0.0172 - accuracy: 0.9908 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
      "Epoch 27/36\n",
      "91/91 [==============================] - 46s 506ms/step - loss: 0.0284 - accuracy: 0.9917 - val_loss: 0.0044 - val_accuracy: 1.0000\n",
      "Epoch 28/36\n",
      "91/91 [==============================] - 46s 506ms/step - loss: 0.0164 - accuracy: 0.9945 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
      "Epoch 29/36\n",
      "91/91 [==============================] - 46s 507ms/step - loss: 0.0174 - accuracy: 0.9926 - val_loss: 0.0111 - val_accuracy: 0.9891\n",
      "Epoch 30/36\n",
      "91/91 [==============================] - 46s 505ms/step - loss: 0.0269 - accuracy: 0.9917 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 31/36\n",
      "91/91 [==============================] - 46s 507ms/step - loss: 0.0076 - accuracy: 0.9972 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 32/36\n",
      "91/91 [==============================] - 46s 507ms/step - loss: 0.0127 - accuracy: 0.9963 - val_loss: 0.0069 - val_accuracy: 0.9964\n",
      "Epoch 33/36\n",
      "91/91 [==============================] - 46s 508ms/step - loss: 0.0212 - accuracy: 0.9926 - val_loss: 0.0120 - val_accuracy: 0.9964\n",
      "Epoch 34/36\n",
      "91/91 [==============================] - 46s 507ms/step - loss: 0.0191 - accuracy: 0.9945 - val_loss: 0.0084 - val_accuracy: 0.9964\n",
      "Epoch 35/36\n",
      "91/91 [==============================] - 46s 506ms/step - loss: 0.0201 - accuracy: 0.9908 - val_loss: 0.0103 - val_accuracy: 0.9964\n",
      "Epoch 36/36\n",
      "91/91 [==============================] - 46s 505ms/step - loss: 0.0219 - accuracy: 0.9917 - val_loss: 0.0153 - val_accuracy: 0.9928\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fae2c0172d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(aug.flow(x_train, y_train, batch_size = BS),\n",
    "         steps_per_epoch = len(x_train)//BS,\n",
    "         validation_data = (x_test, y_test),\n",
    "         validation_steps = len(x_test)//BS,\n",
    "         epochs = Epochs\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fixed-player",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mukul/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /home/mukul/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: facemask.model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(r\"facemask.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "registered-dance",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "preds = model.predict(x_test, batch_size = BS)\n",
    "preds = np.argmax(preds, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "metric-neutral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   with_mask       0.99      0.99      0.99       138\n",
      "without_mask       0.99      0.99      0.99       138\n",
      "\n",
      "    accuracy                           0.99       276\n",
      "   macro avg       0.99      0.99      0.99       276\n",
      "weighted avg       0.99      0.99      0.99       276\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test.argmax(axis = 1),preds, target_names = lb.classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
